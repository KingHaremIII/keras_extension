# ComplexNet文档 #------**version1.0***Ziyao Hong***28 Spe. 2019**## 脚本清单 ##---> 粗体表示文件夹，非粗体表示脚本(.py)+ **keras_extension**    + **ComplexNet**        + complexdense        + complexconvolution    + KerasTensorOperationLayers该程序包包含三个脚本：1. complexdense.py实现了复数全连接层的功能，并且进一步封装了一个复数全连接网络（complexnet）的快速构建网络接口。2. complexconvolution.py实现了复数卷积层的功能。3. KerasTensorOperationLayers.py主要通过匿名层（Lambda，见下面__补充：快速构建_keras_层——匿名层的使用__）集成了一些基础_tensor_操作（切片、批次矩阵相乘等），由于_keras_建模要求每个tensor都来自于一个_keras_层，直接对_tensor_进行操作时_keras_建模会失败，所以必须将这些操作封装成_keras_层。## _keras_层编写综述 ##---([_keras_官方文档中关于编写自己的层描述](https://keras.io/layers/writing-your-own-keras-layers/))_keras_层全部继承或间接继承自一个抽象类——Layer，这是一个keras层的原型，主要包含了初始化方法（\_\_init\_\_）、构建方法（build）和调用方法（call）。```pythonclass Layer(Object):    def __init__(<parameters>):    def build(<parameters>):    def call(<parameters>):    def compute_output_shape(<parameters>):```如上所示，一个keras层有最核心的四个成员方法，如果没有特殊要求，一般对这四个成员方法改写即可，其他方法直接继承自Layer层。 ### 初始化（\_\_init\_\_） ###\_\_init\_\_方法初始化一个类实例，其参数一般包含但不限于：```python    self, units, activation, use_bias,    kernel_initializer, bias_initializer,    kernel_regularizer, bias_regularizer, activity_regularizer    kernel_constraint, bias_constraint    **kwargs```+ 第一行的`self`是每个类方法必备的参数，指代类本身，`units`是本层输出维度（如全连接层代表输出神经元数）。    + 但是实际上除`self`外的其他参数都是可以改变的，如卷积层里没有`units`参数，多了`filters`（输出信道数）和`kernel_size`（卷积核大小）。在`Layer`原型中就只有`self`和`kwargs`两个参数。`def __init(self, **kwargs):`    + `activation`是设置激活函数的接口， `use_bias`是设置是否使用偏置的接口，该参数是个布尔型变量，只能设为`False`或`True`。+ 第二行分别是本层的核（即权重，keras中称其为核，而把权重和偏置统称为权重）和偏置的初始化接口，在`keras.initializers`中有详细的初始化类及其说明。一般全连接层默认He初始化，卷积默认Xiver初始化。如果需要调用其他的初始化类或者要改写，可以去了解`keras.initializers`底层代码。+ 第三行分别是核、偏置和输出值的正则化方式，详细和见`keras.regularizers`。+ 类似的第四行分别是核和偏置的限制，如限制非负或者行（列）的模2等于1等等。+ 最后一行是一个字典型变量，用于传输一些可省缺变量，常见的有指定输入张量形状的`input_shape=(2, 3)`这种用法。（__注意__：输入这些省缺变量时无需使用字典形式即`{'input_shape': (2, 3)}`这种形式）### 构建（build） ###`build`函数主要完成权重的初始化和添加工作，一些逻辑判定也在此方法完成。`build`一般接收两个参数——`self`和`input_shape`（`def build(self, input_shape):`），后者主要用于根据输入张量形状判断权重的形状。### 调用——计算内核（call） ###_keras_层都是可调用的类，通过`__call__(<parameters>)`这一特殊API实现，可调用的类实例可以当作一个函数使用：```python# 创建一个CertainClass类的实例instance = CertainClass(<初始化参数>)# 下面的语句将直接执行instance的__call__方法value = instance(<__call__函数的参数>)```而_keras_更进一步将`__call__`封装：```pythondef __call__(self, <parameters>):    # 完成一些设置和判断    <certain codes>    # 通过一些语句调用call方法    <certain codes> self.call(<parameters if any>) <certain codes>```二级开发者只需重写原本为空函数的`call`即可（如全连接层，其`call`函数完成输入张量`input`与核`W`的矩阵乘积运算），底层操作继承`__call__`。__注意__：可调用类的好处仅仅在于方便操作，可以直接匿名地调用某些操作。### 补充：快速构建_keras_层——匿名层（Lambda）的使用 ###为了简化一些简单层的开发，_keras_提供了类似于普通`python`中的匿名函数（lambda）的匿名层（`keras.layers.Lambda`），可以看作是tensor的匿名函数。使用`Lambda`开发新的_keras_层极为简便，开发者只需构建计算内核，其一般形式如下：```pythonLambda(target function, arguments=<arguments if any>, name=<name if needed>)(target tensor)```其中`target function`完成我们所需的对目标tensor的操作功能，可以是一个匿名函数如：`lambda x: K.int_shape(x)`（输出tensor的形状），也可以是一个显式定义的函数。以下给出中的一个例子，该例子封装了一个函数，用于接收构建这个`Lambda`层的参数，可以作为开发`Lambda`层的一般过程：```pythondef Expand_dims(tensor, position, name=None, fast_index=None):    '''expand dimenion in <position>       在指定维度上插入一个新的维度，如原张量形状为：(2, 3, 4)，通过指定参数position=1，则输出新张量的形状为：(2, 1, 3, 4)。       参数tensor是目标张量，position参数是目标维度，name是该层名字fast_index提供了一种快速命名方式，可以省缺name而命名为"Expand_dims"+fast_index的名字。       该张量操作一般用于张量的合并（concatenate）前操作，在指定维度上扩充后再在该维度合并。    '''    # 该层的命名逻辑操作，默认为"Expand_dims"    if name is None:        name = 'Expand_dims'        if fast_index is not None:            name = name + str(fast_index)    elif type(name) is not str:        raise ValueError('argument <name> should be a string or "None".')    # 逻辑判断，扩展维度序号不能超出维度序号上限    if position > len(tensor.shape):        raise ValueError('Dimension for expansion exceeded the sup')    # 因为功能简单，这里使用了匿名函数作为Lambda层的目标函数    return Lambda(lambda x: K.expand_dims(x, position), name=name)(tensor)```### 编写_keras_层简单实例 ###以下是一个简单的自己编写的_keras_层实例：```pythonclass GuideLayer(Layer):    '''这个实例层构建一个指定<units>阶的方阵，并和输入张量做克罗内克积    '''    def __init__(self, units, **kwargs):        # 打印一串信息，确认进入了初始化方法        print('This is a Guide Layer for DIY keras layers')        # 接收units参数并构建类参数GuideLayer.units        self.units = units        # super方法是一个常见的调用父类方法的手段，下面的语句以kwargs为参数调用了GuideLayer的父类Layer的初始化方法。        super().__init__(**kwargs)    def build(self, input_shape):        ''''这里的input_shape实际没有用到        '''        # 构建核的形状，(units, units)        self.kernerl_shape = (self.units, self.units)        # 创建核，add_weight是继承自Layer的一个底层方法，开发者只需指定参数，实际上还有限制(constraint)和        #     初始化(initializer)等参数，这里直接使用默认。        self.kernel = self.add_weight(shape=self.kernerl_shape, name='kernel')        # built是一个逻辑变量，提示是否构建过，一般默认使用下面语句即可        self.built = True    def call(self, inputs):        '''计算内核：inputs⊙kernel        '''        # 导入必要的tensorflow进行克罗内克积的API（keras后端不提供该操作）        from tensorflow.linalg import LinearOperatorKronecker, LinearOperatorFullMatrix        # 返回克罗内克积结果，其中.to_dense()将稀疏阵稠密化        return LinearOperatorKronecker(                [LinearOperatorFullMatrix(inputs),                 LinearOperatorFullMatrix(sefl.kernel)]).to_dense()    def compute_output_shape(self, input_shape):        '''计算输出张量形状，必须是一个元组（tuple）或元组组成的列表（多输出情况下）        '''        # 输出形状为：(N*self.units, M*self.units)，则将输入形状都乘以self.units即可        # 首先将input_shape转化为numpy数组，因为tuple类型不能分别赋值        output_shape = np.array(input_shape)        # 改变形状，利用了numpy的广播功能        output_shape = output_shape * self.units        # 将numpy数组转化为tuple输出        return tuple(output_shape)```编写好的层可以和_keras_固有的层一样使用：```pythonfrom keras import Input, Modelinput_tensor = Input((3, ))output_tensor = GuideLayer(5, )(input_tensor)model = Model(input_tensor, output_tensor)```__注意__：为了简洁明了，上述例子程序不具有鲁棒性，缺少必要的逻辑判断，如在build中添加对输入张量形状的判断，如果不是一个二维张量（矩阵）而是大于二维的张量应该及时报错，避免在程序运行时才产生错误，给调试带来麻烦。## 复数全连接层 ##---complexdense.py脚本包含> **ComplexDense**：复数全连接层类> **ComplexDenseNet**：基于复数全连接层的级联网络### 复数全连接层 ###> 使用指南```python    ComplexDense(units,                 activation=None,   # support LeakyReLU by specify                                    # 'LReLU' or 'LeakyReLU' or 'leakyrelu'                 leakyrelu_alpha=0.3,                 use_bias=True,                 init_criterion='he',    # 'glorot'                 kernel_initializer='complex',                 bias_initializer='zeros',                 kernel_regularizer=None,                 bias_regularizer=None,                 activity_regularizer=None,                 kernel_constraint=None,                 bias_constraint=None,                 seed=None,                 output_merge=False,                 **kwargs):```复数卷积层的使用和实数卷积层使用上机会一致，这个类主要多了两个地方：1. 集成了`LeakyReLU`激活函数（见activation的注释）。leakyrelu\_alpha参数设置`LeakyReLU`函数的alpha值。2. 对于输出形状的控制（`output_merge`参数）    1. 该参数为`True`时将输出合并为一个张量，这个张量的第二个维度（维度1）是实部和虚部信道，例如输出的是张量a，形状为（None, 2, 3, 4），则`a[:, 0, :, :]`是实部，`a[:, 1, :, :]`是虚部。    2. 该参数为`False`时将输出一个包含两个同样大小张量的列表，第一个张是实部，第二个张量是虚部。__注意__：该_keras_层是一个双输入层，需要将实部和虚部放在一个列表中作为该层的输入，输出可以是单输出或双输出。```pythonoutput_tensor = ComplexDense(<parameters>, output_merge=True)([real_part, image_part])```or```python[output_real, output_imag] = CompelxDense(<parameters>, output_merge=False)([real_part, image_part])```### 复数全连接层网络 ###> 使用指南```python ComplexDenseNet(No_, layers, units,                 output_merge=False,                 activation=None,                 leakyrelu_alpha=0.3,                 use_bias=True,                 init_criterion='he',    # 'glorot'                 kernel_initializer='complex',                 bias_initializer='zeros',                 kernel_regularizer=None,                 bias_regularizer=None,                 activity_regularizer=None,                 kernel_constraint=None,                 bias_constraint=None,                 seed=None,                 **kwargs)```这个网络类是对复数全连接层的封装，可以指定层数和每层的神经元数，目前的版本中其他参数只能使用统一的设置。1. `No_`这个参数是为了区分各个复数全连接网络的唯一识别符，应输入一个字符串<str>。2. `layers`这个参数指定层输，是个整型变量<int>。3. `units`可以是单个值，这是所有层将设置为同样大小，也可以传入一个由整型变量组成的列表（如`units=[2, 3, 4]`），分别设置每层的神经元数。__注意__：要么统一使用同一个units，要么为每个层设置神经元数，不支持省缺形式。（如：`layers=3, units=[2, 3]`是错误使用方法，逻辑模块会直接报错）4. `output_merge`和`activation`与复数全连接层说明相同。## 复数卷积层 ##---complexconvolution.py脚本包含> **_ComplexConv**：复数卷积层> **ComplexConv1D**：以_ComplexConv为父类的一维复数卷积层### 复数卷积层 ###> 该层作为复数卷积层的底层驱动，一般不直接调用。### 复数一维卷积层 ###> 使用指南```python   ComplexConv1D(filters,                 kernel_size,                 strides=1,                 padding='valid',                 data_format='channels_last',                 dilation_rate=1,                 activation=None,                 use_bias=True,                 kernel_initializer='glorot_uniform',                 bias_initializer='zeros',                 kernel_regularizer=None,                 bias_regularizer=None,                 activity_regularizer=None,                 kernel_constraint=None,                 bias_constraint=None,                 output_merge=False,                 **kwargs)```类似地，复数一维卷积层的`output_merge`参数用于控制输出形式。其他参数与实数一维卷积层一致。## 封装的tensor操作层 ##---KerasTensorOperationLayers.py包含：> **Slice_tensor**：对张量进行切片操作，如`a[:, 0:2, :]`对张量a进行了在维度1上的切片，取出了维度1上第0和1行（或者说列，高维张量里行列名称没有意义了）。> **Expand_dims**：在指定维度上扩展一维，如形状为（2, 3）的张量在第1维（从0开始）上扩展，形状变为（2, 1, 3）。> **Concetanate_tensor**：在指定维度上拼接张量，如a和b是两个同为（2, 1, 3）形状的张量，在维度1上拼接成为（2, 2, 3）形状的新张量。> **Expand_concetanate_tensor**：Expand_dims和Concetanate_tensor的集成，使用该API可以避免创建太多的层，是网络看起来过于复杂。> **Dot**：矩阵相乘API，可以指定是直接将张量做矩阵乘法还是在批次内做每个对应矩阵的乘法，例如a和b是两个形状分别为（2, 3, 4）和（2, 4, 3）的张量，指定使用批次内矩阵乘法，则是a的第一（3, 4）的矩阵和b的第一个（4, 3）的矩阵做矩阵乘法，接着a的第二（3, 4）的矩阵和b的第二个（4, 3）的矩阵做矩阵乘法，即`tf.matmul(a[0], b[0]), tf.matmul(a[1], b[1])`。> **matrix_wise_transpose**：将一个三维张量看作是一批矩阵，对这些矩阵做转置，例如a是一个形状为（None, 3, 4）的张量，可以将每个（3, 4）的矩阵转置，从而a变成形状为（None, 4, 3）的张量。`该脚本中均为函数，需要的话也可以封装成类。`以下`name`和`fast_index`和__补充：快速构建_keras_层——匿名层的使用__中叙述一致。### 张量切片API ###> 使用指南```pythonSlice_tensor(tensor, position, slice_, name=None, fast_index=None,             keepdims=False)````tensor`为目标张量，`position`是操作的维度，`keepdims`如果切割的结果在指定维度上只剩一个元素，是否丢掉这个维度或者保持该维度上一个元素的形式输出。### 张量维度扩展API ###> 使用指南```pythonExpand_dims(tensor, position, name=None, fast_index=None)```说明与上述一致。### 张量拼接API ###> 使用指南```pythonConcetanate_tensor(tensors, position, name=None, fast_index=None)```说明与上述一致。### 张量维度扩展拼接集成API ###> 使用指南```pythonExpand_concetanate_tensor(tensors, position,                              name=None, fast_index=None)```说明与上述一致。### 张量矩阵乘法API ###> 使用指南```pythonDot(tensorx, tensory, model='dot',    # 'batch_dot'        name=None, fast_index=None)````tensorx`和`tensory`分别是两个待乘的张量，`model`控制乘法形式，只能取`dot`或`batch_dot`，分别表示做普通张量乘法或批次内张量乘法。如：```pythona = Input((2, 3, 4))    # a is a tensor with shape (None, 2, 3, 4)b = K.constant(np.zeros((7, 5, 2)))    # b is a tensor with shape (7, 4, 5) c = Dot(a, b, model='dot')    # c is a tensor with shape (None, 2, 3, 7, 5)aa = Input((2, 3, 4))bb = Input((2, 4, 5))d = Dot(a, b, model='batch_dot')    # d is a tensor with shape (None, 2, 3, 5)````Dot`的实现主要基于`K.dot`和`K.batch_dot`这两个底层tensor操作接口，其区别可见[链接](https://blog.csdn.net/huml126/article/details/88739846)。该API的作用更多的是用于两个三维张量每个矩阵对应矩阵乘法运算：```pythona = Input((3, 4))b = Input((4, 5))c = Dot(a, b, dot='batch_dot')    # c is a tensor with shape (None, 3, 5)```### 批次内张量转置API ###> 使用指南```pythonmatrix_wise_transpose(tensors, name=None, fast_index=None)```# Code Using Guide #------## ComplexDense ##---```python        from keras import Input, Model        input_real = Input((3, ), name='input_real')        input_imag = Input((3, ), name='input_image')        # 中间复数全连接层一定是输出不合并，方便下面调用        [output_r, output_i] = ComplexDense(                units=2, output_merge=False,                name='TestCDense1', activation='leakyrelu')([input_real,                                                             input_imag])        [output_r, output_i] = ComplexDense(                units=3, output_merge=False,                name='TestCDense2', activation='leakyrelu')([output_r,                                                             output_i])        output_tensor = ComplexDense(                units=5, output_merge=True,                name='TestCDense3', activation='leakyrelu')([output_r,                                                             output_i])        # 最终输出合并        model = Model([input_real, input_imag], output_tensor,                      name='TestModel')        model.compile(optimizer='adam', loss='mse')        print('Merged Version: \n')        print(model.summary())       # 最终输出不合并        [output_r, output_i] = ComplexDense(                units=5, output_merge=False,                name='TestCDense', activation='leakyrelu')([input_real,                                                            input_imag])        model = Model([input_real, input_imag], [output_r, output_i],                      name='TestModel')        model.compile(optimizer='adam', loss='mse')        print('Not Merged Version: \n')        print(model.summary())```## ComplexConv1D ##---```python        from keras import Input, Model        input_real = Input((3, 1), name='channellastReal1D')        input_imag = Input((3, 1), name='channellastImag1D')        CConv1d = ComplexConv1D(                5, kernel_size=(3, ), use_bias=False,                output_merge=False,                name='ComplexConv1D')        output_tensor = CConv1d([input_real, input_imag])        model = Model([input_real, input_imag], output_tensor,                      name='TestModel')        model.compile(optimizer='adam', loss='mse')        print(model.summary())```